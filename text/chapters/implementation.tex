\chapter{Implementation in NetworKit}
\label{ch:implementation}

We have implemented the dynamic algorithms for Top-$k$ closeness centrality and group closeness from this thesis in NetworKit. NetworKit is a self-described ``open-source toolkit for high-performance network analysis'' \cite{staudt2014networkit}. Most of the code is written in C++, while a Cython~\cite{behnel2010cython} wrapper provides a Python interface. This arrangement allows for implementations with minimal overhead while still providing a simple interface for end users.

\section{Existing components}

NetworKit provides tools to import graphs from files in several formats. There is a unified interface for graphs that provides (parallel) iterators for nodes, edges and neighbors. On top of that, NetworKit provides data structures that are often required for graph algorithms, such as priority queues.

In NetworKit, dynamic algorithms hold a reference to the underlying graph. Once the graph is modified externally, the dynamic algorithm is notified with a \texttt{GraphEvent} that contains the necessary information. For edge modifications, the event signals whether it was an edge insertion or an edge removal and contains the pair of nodes that is part of the edge.

There are already implementations of the static algorithms for Top-$k$ closeness centrality in complex networks and street networks in NetworKit. There also is an implementation for the algorithm for group closeness by Bergamini et al. Our dynamic algorithms for these problems are based on these implementations.

\section{New components}
We add four new algorithms to NetworKit. \texttt{DynTopClosenessHarmonic} manages the list of the $k$ most central nodes in a dynamic graph. \texttt{DynGroupCloseness} approximates the group with the highest group closeness among all groups with $k$ members in a dynamic graph. 

The class \texttt{AffectedNodes} is used by both dynamic algorithms to compute the set of nodes affected by an edge modification. On top of that, \texttt{AffectedNodes} computes an upper bound for the improvement of the closeness centrality for each affected node. Recall from Section~\ref{sec:levelBasedImprovementBounds} that computing these upper bounds only requires knowing the distances to the edge modification for each node. Since computing the set of affected nodes already requires a complete BFS from the affected nodes incident to the modified edge, it is possible to compute the distance to the edge modification for each reachable node.

At last, we added a class \texttt{DynConnectedComponents} which keeps track of the connected components of an undirected graph. In the case of an edge insertion, we check whether the two nodes are in the same connected component. If this is the case, we have to do nothing else. If the nodes are in different components, we merge the components. In the case of an edge removal, we simply re-run Algorithm~\ref{alg:connectedComponents}. We do not aim to solve the problem of keeping track of (strongly) connected components in dynamic graphs in general, but rather optimize the easiest case.

\section{Implementation details and parallelism}
\paragraph{Top-$k$ closeness centrality}
The algorithm for Top-$k$ closeness centrality in complex networks is almost \emph{embarrassingly parallel}. Recall that the algorithm runs a pruned BFS from each (affected) node in the graph. These breadth-first searches are all independent from each other and can be run in parallel. The cutoff level for these searches depends on the closeness centrality $x_k$ of the $k$-th most central nodes. The $k$ most central nodes are managed by a shared priority queue. Accesses to this priority queue must be protected by a mutex. Since multiple pruned searches are run in parallel, it is possible that the $k$-th most central node changes while other searches are still running. It makes sense to keep track of $x_k$ for each thread individually and update it with the global value after each local iteration. While it would be possible to share $x_k$ between all threads, this would introduce a lot of synchronization overhead. In practice, $x_k$ rarely changes by a huge amount and thus the gains of updating~$x_k$ ``live'' are negligible. As a result, the parallel implementation usually computes the exact closeness centrality and tighter upper bounds for more nodes than a single-threaded implementation because $x_k$ is only updated roughly every $n$ iterations (with $n$ being the number of threads).

The nodes for which a pruned BFS has yet to be run are also managed by a priority queue. In our implementation, we sort nodes by their degree for the run on the initial graph. After edge modifications, we build a queue of affected nodes sorted by their previous closeness centrality (upper bound). These heuristics aim to process nodes with a potentially high closeness centrality first. The higher the closeness centralities of the early nodes, the larger $x_k$ will be and the earlier the breadth-first searches of less-central nodes can be cut off. The algorithm for large diameters actually requires reordering the priority queue when a tighter upper bound is obtained for any node. In any case, the priority queue is shared among all threads and accesses to it have to be synchronized with a mutex.

Additionally, our algorithm keeps track of the current upper bounds and exact closeness centralities for each node in a global vector. It also keeps track of the cutoff level of the last pruned BFS executed for each node and whether the stored value is only an upper bound for the closeness centrality. In the algorithm for complex networks, one node is only processed by one corresponding thread. Therefore, synchronization is not required when accessing these shared data structures. The algorithm for networks with large diameters, however, can obtain upper bounds for multiple nodes with only one BFS. In this case, we protect accesses to any of these data structures with a single mutex in order to ensure consistency.

\paragraph{Group closeness}
Recall that the algorithm that approximates the group with the highest group closeness computes $\Delta_i(u) = c(S_{i - 1} \cup \{u\})$ for $u \in V \setminus S_{i - 1}$ in each iteration~$i$. For the dynamic algorithm, we need to keep a list of all nodes $u$ sorted by $\Delta_i(u)$ for each iteration. In case $\Delta_i(u)$ is recomputed after an edge insertion, we need to update this list for all iterations. We use $k$ priority queues, one for each iteration.

We first compute $\Delta_i(u)$ in parallel and store the results locally. After all the nodes are processed, we update the priority queue for the corresponding iteration with the new values. After an edge insertion, we only need to recompute $\Delta_i(u)$ for unaffected nodes $u$ if $u$ is actually the first node in the priority queue, that is the node with the largest improvement among all nodes. Our algorithm extracts nodes from the priority queue, recomputes their exact improvement if necessary and then adds the node back to the queue. This is repeated until the exact value for $\Delta_i(u)$ is known for the first node in the queue.

\paragraph{Affected nodes}
When computing the set of affected nodes of an edge modification $(u, v)$ in undirected graphs, the breadth-first searches starting in $u$ and $v$ are independent from each other and can also be run in parallel.

